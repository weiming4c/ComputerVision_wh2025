{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad81b19",
   "metadata": {},
   "source": [
    "# Machine Learning (L4&5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6197613",
   "metadata": {},
   "source": [
    "# 机器学习/深度学习的标准流程\n",
    "\n",
    "课程通过一个“判断手写数字是否为5”的二分类任务，系统地介绍了监督学习的六大步骤：\n",
    "\n",
    "1.  设立任务 (Set up the task)：明确目标，例如图像分类。\n",
    "2.  准备数据 (Prepare the data)：需要一个带标签的数据集，如MNIST。\n",
    "3.  构建模型 (Build a model)：选择一个数学模型来拟合输入（图像）到输出（标签）的关系。\n",
    "4.  设定目标 (Decide the objective)：定义一个**损失函数 (Loss Function)**，用来衡量模型预测的好坏。\n",
    "5.  执行拟合 (Perform fitting)：通过**优化算法 (Optimization)**，调整模型参数，使得损失函数最小化。\n",
    "6.  测试评估 (Testing)：在未见过的新数据上评估模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76b017",
   "metadata": {},
   "source": [
    "## 基础模型\n",
    "\n",
    "*   **模型演进**：\n",
    "    1.  **逻辑回归 (Logistic Regression)**：作为一个最简单的“神经网络”，它将图像“压平”成一个长向量，通过一个线性变换和Sigmoid激活函数，输出一个0到1之间的概率值。\n",
    "    2.  **多层感知机 (Multi-Layer Perceptron, MLP)**：通过堆叠多个线性层和非线性激活函数（如ReLU），MLP可以学习复杂的非线性关系，解决逻辑回归无法处理的“线性不可分”问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fa98b",
   "metadata": {},
   "source": [
    "### 逻辑回归 (Logistic Regression)\n",
    "逻辑回归: 一个“单层神经网络”\n",
    "\n",
    "任务范例：判断一张 28x28 像素的手写数字图像是否为数字“5”。这是一个典型的“是”或“否”的问题。\n",
    "\n",
    "#### 模型构建三部曲\n",
    "逻辑回归模型的构建过程可以清晰地分为三步，这三步也体现了神经网络单个神经元的基本工作原理。\n",
    "1. 步骤一：输入处理 —— 压平 (Flatten)\n",
    "2. 步骤二：核心计算 —— 线性加权求和 (Linear Combination)\n",
    "操作：用一个权重向量 θ 与输入向量 x 进行点积运算（z = θ^T * x），有时还会加上一个偏置项 b。  \n",
    "直观理解：这相当于给每个像素的重要性赋予一个权重。例如，如果图像中心区域的像素对于判断“5”更重要，那么对应位置的权重 θ 值就会更大。这个计算结果 z 可以看作是一个“得分”。\n",
    "3. 步骤三：输出转换 —— Sigmoid激活函数\n",
    "问题：线性计算得到的“得分” z 是一个从负无穷到正无穷的实数，而我们希望得到的是一个表示“是‘5’的概率”，这个概率值应该在 [0, 1] 区间内。  \n",
    "解决方案：使用 Sigmoid 函数（也叫Logistic函数），其公式为 g(z) = 1 / (1 + e^(-z))。  \n",
    "作用：它能将任何实数 z “挤压”到 (0, 1) 的范围内。输出值可以被解释为概率，例如，输出0.99就代表模型有99%的把握认为这张图是“5”。\n",
    "\n",
    "**模型总结：逻辑回归的完整模型就是这三步的结合，即 预测概率 = Sigmoid(权重 * 输入向量 + 偏置)。**\n",
    "\n",
    "#### 几何直觉与局限性\n",
    "- 决策边界 (Decision Boundary)：逻辑回归的决策边界是线性的。在二维空间里是一条直线，在三维空间里是一个平面，在高维空间里则是一个“超平面”。它只能用“一刀切”的方式来划分数据。\n",
    "- 核心局限：它无法解决线性不可分 (Linearly Non-separable) 的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbba43",
   "metadata": {},
   "source": [
    "### 多层感知机 (MLP) \n",
    "\n",
    "MLP通过堆叠多个“神经元层”来克服逻辑回归的线性限制，是真正意义上的“神经网络”。\n",
    "\n",
    "*   **核心思想**：如果一层（一个线性分割）不够，那就用多层。通过多个层级的复杂变换，MLP可以将原始空间中线性不可分的数据，映射到一个新的、更高维的空间，在这个新空间里数据就变得**线性可分**了。\n",
    "\n",
    "#### MLP的结构解析\n",
    "\n",
    "一个典型的MLP由三部分构成：\n",
    "\n",
    "*   **输入层 (Input Layer)**：接收原始数据（和LR一样，是压平后的图像向量）。\n",
    "*   **隐藏层 (Hidden Layers)**：可以有一层或多层。每一层都执行两个关键操作：\n",
    "    1.  **线性变换**：与逻辑回归类似，对上一层的输出进行加权求和 (`W*x + b`)。\n",
    "    2.  **非线性激活 (Non-linear Activation)**：对线性变换的结果应用一个**非线性激活函数**,使得它的图像不再是一条直线。\n",
    "*   **输出层 (Output Layer)**：产生最终的预测结果。\n",
    "\n",
    "##### 非线性激活函数的作用\n",
    "\n",
    "*   **为什么必须是非线性？** 如果没有非线性激活函数，仅仅是堆叠多个线性层，其最终效果和一个单层的线性模型是完全等价的（因为一系列线性变换的组合仍然是一个线性变换）。这样的网络无论有多少层，都只能解决线性问题。\n",
    "*   **“掰弯”决策边界**：正是非线性激活函数的引入，才使得每一层都能对数据进行一次“弯曲”或“折叠”，多层组合起来就能拟合出任意复杂的决策边界，从而解决像“同心圆”这样的非线性问题。\n",
    "*   **常见的激活函数**：课程中提到了Sigmoid，但更强调了现代神经网络中更常用的**ReLU (Rectified Linear Unit)** 函数，它计算简单且能有效缓解梯度消失问题，是目前最主流的选择。\n",
    "\n",
    "#### MLP的训练与反向传播\n",
    "\n",
    "*   **训练流程**：与逻辑回归完全一致，遵循“**前向传播 → 计算损失 → 反向传播 → 更新参数**”的循环。\n",
    "*   **新的挑战**：MLP的参数（各层的权重W和偏置b）数量远多于逻辑回归，如何高效计算损失函数对每一个参数的梯度（导数）成了一个难题。\n",
    "*   **解决方案：反向传播 (Backpropagation)**\n",
    "    *   **本质**：一种利用**链式法则**高效计算梯度的算法。\n",
    "    *   **直观理解**：它首先计算出损失对网络最后一层输出的梯度，然后像多米诺骨牌一样，逐层向后“传播”这个梯度，计算出损失对前一层输出的梯度，并利用这个结果计算当前层参数的梯度，直到第一层为止。\n",
    "    *   **意义**：反向传播是所有深度学习模型训练的核心算法，使得训练深度、复杂网络成为可能。\n",
    "\n",
    "#### MLP处理图像的根本缺陷\n",
    "\n",
    "课程在讲解完MLP后，再次点明了它作为通用模型在应用于图像时的两大硬伤，这也是转向CNN的直接原因：\n",
    "\n",
    "1.  **参数量爆炸**：由于隐藏层是**全连接 (Fully-connected)** 的（即每个神经元都与前一层的所有神经元相连），当图像分辨率增大时，参数数量会急剧膨胀，导致计算成本极高且容易过拟合。\n",
    "2.  **空间结构丢失**：最初的“压平”操作彻底破坏了像素之间的邻里关系，而这种空间局部性恰恰是理解图像内容的关键。\n",
    "\n",
    "#### **总结与对比**\n",
    "\n",
    "| 特征 | 逻辑回归 (Logistic Regression) | 多层感知机 (MLP) |\n",
    "| :--- | :--- | :--- |\n",
    "| **模型结构** | 单层网络（输入直接到输出） | 多层网络（包含一个或多个隐藏层） |\n",
    "| **核心计算** | 线性变换 + Sigmoid激活 | (线性变换 + **非线性激活**) 的多次堆叠 |\n",
    "| **决策边界** | **线性** (直线/平面/超平面) | **非线性** (任意复杂形状) |\n",
    "| **解决问题** | 仅限线性可分问题 | 可解决复杂的非线性问题 |\n",
    "| **与图像任务的适配性** | 差：破坏空间结构，参数量尚可（因层数少） | **极差**：破坏空间结构，且参数量巨大 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ce928",
   "metadata": {},
   "source": [
    "## 优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d695b34",
   "metadata": {},
   "source": [
    "*   **核心数学工具**：\n",
    "    *   **损失函数**：课程推导了**负对数似然损失 (Negative Log-likelihood, NLL)**，这是分类任务中最常用的损失函数之一，其背后思想是最大化“观测到当前标签数据的概率”。\n",
    "    *   **优化算法**：介绍了**梯度下降 (Gradient Descent, GD)**。即沿着损失函数梯度（导数）的反方向更新模型参数，以最快速度找到损失的最小值。\n",
    "    *   **训练技巧**：对比了**批量梯度下降 (Batch GD)** 和 **随机/小批量梯度下降 (SGD)** 的优劣。SGD计算速度快，且其随机性有助于跳出“局部最优”，是训练大型网络的标准做法。\n",
    "    *   **参数更新的核心**：**反向传播 (Backpropagation)**。这是一种利用链式法则高效计算复杂网络中每个参数梯度的算法，是所有深度学习框架的基石。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd0507",
   "metadata": {},
   "source": [
    "### 负对数似然损失（NLL）  \n",
    "**核心目标是：让模型预测的概率分布尽可能地接近真实的标签分布。**\n",
    "\n",
    "- 如果真实标签是 $ y = 1 $（是“5”），我们希望模型预测 $ p(y=1 \\mid x; \\theta) $ 接近 1。  \n",
    "- 如果真实标签是 $ y = 0 $（不是“5”），我们希望模型预测 $ p(y=1 \\mid x; \\theta) $ 接近 0，即 $ p(y=0 \\mid x; \\theta) = 1 - p(y=1 \\mid x; \\theta) $ 接近 1。\n",
    "\n",
    "---\n",
    "\n",
    "#### **负对数似然损失 (NLL) 的逻辑推导**\n",
    "\n",
    "我们的最终目标是找到一组最佳的模型参数 $ \\theta $，使得在给定所有训练图片 $ X $ 的情况下，观测到它们对应的所有真实标签 $ Y $ 的**联合概率**最大化。这个原则被称为 **最大似然估计（Maximum Likelihood Estimation, MLE）**。\n",
    "\n",
    "---\n",
    "\n",
    "##### **1. 单个数据点的概率建模**\n",
    "\n",
    "考虑一个样本 $ (x^{(i)}, y^{(i)}) $：\n",
    "\n",
    "- 模型输出 $ h_\\theta(x^{(i)}) $ 表示：  \n",
    "  $$\n",
    "  h_\\theta(x^{(i)}) = p(y^{(i)} = 1 \\mid x^{(i)}; \\theta)\n",
    "  $$\n",
    "- 因此，类别 0 的概率为：  \n",
    "  $$\n",
    "  p(y^{(i)} = 0 \\mid x^{(i)}; \\theta) = 1 - h_\\theta(x^{(i)})\n",
    "  $$\n",
    "\n",
    "我们可以用一个统一的公式表示该样本的条件概率：\n",
    "\n",
    "$$\n",
    "p(y^{(i)} \\mid x^{(i)}; \\theta) = \\left[ h_\\theta(x^{(i)}) \\right]^{y^{(i)}} \\cdot \\left[ 1 - h_\\theta(x^{(i)}) \\right]^{1 - y^{(i)}}\n",
    "$$\n",
    "\n",
    "✅ **验证：**\n",
    "- 若 $ y^{(i)} = 1 $：  \n",
    "  $$\n",
    "  = h_\\theta(x^{(i)})^1 \\cdot (1 - h_\\theta(x^{(i)}))^0 = h_\\theta(x^{(i)})\n",
    "  $$\n",
    "- 若 $ y^{(i)} = 0 $：  \n",
    "  $$\n",
    "  = h_\\theta(x^{(i)})^0 \\cdot (1 - h_\\theta(x^{(i)}))^1 = 1 - h_\\theta(x^{(i)})\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. 所有数据点的联合概率（似然函数）**\n",
    "\n",
    "假设训练集有 $ n $ 个独立同分布的样本，则整个数据集的**似然函数**为：\n",
    "\n",
    "$$\n",
    "p(Y \\mid X; \\theta) = \\prod_{i=1}^n p(y^{(i)} \\mid x^{(i)}; \\theta) = \\prod_{i=1}^n \\left[ h_\\theta(x^{(i)})^{y^{(i)}} \\cdot \\left(1 - h_\\theta(x^{(i)})\\right)^{1 - y^{(i)}} \\right]\n",
    "$$\n",
    "\n",
    "我们的目标：**最大化这个似然函数**。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. 取对数：对数似然函数**\n",
    "\n",
    "为便于优化（避免连乘、数值下溢、便于求导），对似然函数取自然对数：\n",
    "\n",
    "$$\n",
    "\\log p(Y \\mid X; \\theta) = \\sum_{i=1}^n \\log \\left[ h_\\theta(x^{(i)})^{y^{(i)}} \\cdot \\left(1 - h_\\theta(x^{(i)})\\right)^{1 - y^{(i)}} \\right]\n",
    "$$\n",
    "\n",
    "利用对数性质 $ \\log(ab) = \\log a + \\log b $ 和 $ \\log(a^b) = b \\log a $，得到：\n",
    "\n",
    "$$\n",
    "\\log p(Y \\mid X; \\theta) = \\sum_{i=1}^n \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log \\left(1 - h_\\theta(x^{(i)})\\right) \\right]\n",
    "$$\n",
    "\n",
    "这就是**对数似然函数**。\n",
    "\n",
    "---\n",
    "\n",
    "##### **4. 转换为最小化问题：负对数似然损失（NLL）**\n",
    "\n",
    "机器学习通常**最小化损失函数**，因此我们对对数似然加负号，定义 **负对数似然损失（NLL）**：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\log p(Y \\mid X; \\theta) = -\\sum_{i=1}^n \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log \\left(1 - h_\\theta(x^{(i)})\\right) \\right]\n",
    "$$\n",
    "\n",
    "> 🔔 这个损失函数也称为 **二元交叉熵损失（Binary Cross-Entropy Loss）**。\n",
    "\n",
    "**优化目标**：通过梯度下降等方法，找到 $ \\theta $ 使得 $ \\mathcal{L}(\\theta) $ 最小。\n",
    "\n",
    "---\n",
    "\n",
    "#### **总结与训练流程**\n",
    "\n",
    "在训练过程中，NLL 的使用步骤如下：\n",
    "\n",
    "1. **前向传播**：输入 $ X $，计算预测概率 $ h_\\theta(X) $。\n",
    "2. **计算损失**：代入 NLL 公式：\n",
    "   $$\n",
    "   \\mathcal{L}(\\theta) = -\\sum_{i=1}^n \\left[ y^{(i)} \\log h_\\theta(x^{(i)}) + (1 - y^{(i)}) \\log (1 - h_\\theta(x^{(i)})) \\right]\n",
    "   $$\n",
    "3. **反向传播**：计算 $ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} $。\n",
    "4. **参数更新**：使用优化器（如 SGD、Adam）更新 $ \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L} $。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594a0377",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
